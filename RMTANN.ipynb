{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde1d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597145f0",
   "metadata": {},
   "source": [
    "# define the activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c06e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return (np.exp(x)-1)/(np.exp(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10834d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperbolic_tangent(x):\n",
    "    return (np.exp(2*x)-1)/(np.exp(2*x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c47512",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return np.exp(x)/(np.exp(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ecd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus(x):\n",
    "    return np.log(np.exp(x)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de89f88",
   "metadata": {},
   "source": [
    "# define an one-hidden layer neural network with  r nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bf8af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v is the output weights, which is an r-dimensional vector\n",
    "# w is the input weights, which is an r*n matrix\n",
    "# b is the input biases, which is an r-dimensional vector\n",
    "# d is the output bias, which is a scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2cee0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing, give the initial values of variables, set r = 3, and n = 2.\n",
    "# v = np.array([[1,2,3]]).T\n",
    "# w = np.np.array([[10,2],[2,13],[1,9]])\n",
    "# x = np.array([[0.2,0.5]]).T\n",
    "# b = np.array([[1,-5,0]]).T\n",
    "# d = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c02822",
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network(sigma,v,w,x,b,d):\n",
    "    inner_vector = np.matmul(w,x)+b\n",
    "    if sigma == sigmoid:\n",
    "        return np.matmul(v.T,sigmoid(inner_vector))+d\n",
    "    elif sigma == hyperbolic_tangent:\n",
    "        return np.matmul(v.T,hyperbolic_tangent(inner_vector))+d\n",
    "    elif sigma == logistic:\n",
    "        return np.matmul(v.T,logistic(inner_vector))+d\n",
    "    elif sigma == softplus:\n",
    "        return np.matmul(v.T,softplus(inner_vector))+d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184487fa",
   "metadata": {},
   "source": [
    "# define the matrix A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0658679",
   "metadata": {},
   "source": [
    "## First, define the partial derivatives of the neural network, i.e. Jacobian matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8876dcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_v (sigma,v,w,x,b,d):\n",
    "    inner_vector = np.matmul(w,x)+b\n",
    "    if sigma == sigmoid:\n",
    "        return sigmoid(inner_vector).T\n",
    "    elif sigma == hyperbolic_tangent:\n",
    "        return hyperbolic_tangent(inner_vector).T\n",
    "    elif sigma == logistic:\n",
    "        return logistic(inner_vector).T\n",
    "    elif sigma == softplus:\n",
    "        return softplus(inner_vector).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951cc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsigmoid(x):\n",
    "    return 2*np.exp(x)/(np.exp(x)+1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71117f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dhyperbolic_tangent(x):\n",
    "    return 4*np.exp(2*x)/(np.exp(2*x)+1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c779e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlogistic(x):\n",
    "    return np.exp(x)/(np.exp(x)+1)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef66c915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dsoftplus(x):\n",
    "    return np.exp(x)/(np.exp(x)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527bbff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_w(sigma,v,w,x,b,d,i):\n",
    "    inner_vector = np.matmul(w,x)+b\n",
    "    if sigma == sigmoid:\n",
    "        return (v*dsigmoid(inner_vector)*x[i-1,0]).T\n",
    "    elif sigma == hyperbolic_tangent:\n",
    "        return (v*dhyperbolic_tangent(inner_vector)*x[i-1,0]).T\n",
    "    elif sigma == logistic:\n",
    "        return (v*dlogistic(inner_vector)*x[i-1,0]).T\n",
    "    elif sigma == softplus:\n",
    "        return (v*dsoftplus(inner_vector)*x[i-1,0]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ca7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_b(sigma,v,w,x,b,d):\n",
    "    inner_vector = np.matmul(w,x)+b\n",
    "    if sigma == sigmoid:\n",
    "        return (v*dsigmoid(inner_vector)).T\n",
    "    elif sigma == hyperbolic_tangent:\n",
    "        return (v*dhyperbolic_tangent(inner_vector)).T\n",
    "    elif sigma == logistic:\n",
    "        return (v*dlogistic(inner_vector)).T\n",
    "    elif sigma == softplus:\n",
    "        return (v*dsoftplus(inner_vector)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb30d33",
   "metadata": {},
   "source": [
    "## Define matrix A, which is an \"approximation\" of matrix B=J^TJ, considering the practical meaning of the parameters, we need to consider the problem with triples {v,w,b}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5082d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use np.linalg.norm(N_Î¾,1) to calculate the infinity norm of the row vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59310414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_w_sum(sigma,v,w,x,b,d):\n",
    "    sum_w = 0\n",
    "    for i in range(1,len(x)+1):\n",
    "        sum_w += np.matmul(N_w(sigma,v,w,x,b,d,i).T,N_w(sigma,v,w,x,b,d,i))/np.linalg.norm(N_w(sigma,v,w,x,b,d,i),1)\n",
    "    return sum_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60256668",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix_A(sigma,v,w,x,b,d):\n",
    "    return np.matmul(N_b(sigma,v,w,x,b,d).T,N_b(sigma,v,w,x,b,d))/np.linalg.norm(N_b(sigma,v,w,x,b,d),1)+\\\n",
    "           np.matmul(N_v(sigma,v,w,x,b,d).T,N_v(sigma,v,w,x,b,d))/np.linalg.norm(N_v(sigma,v,w,x,b,d),1)+\\\n",
    "           N_w_sum(sigma,v,w,x,b,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a4b00",
   "metadata": {},
   "source": [
    "# C/F-splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbc1f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for python: https://github.com/pyamg/pyamg/blob/main/pyamg/classical/split.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c5af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference for Julia: https://github.com/JuliaLinearAlgebra/AlgebraicMultigrid.jl/blob/master/src/splitting.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d619685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyamg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9b3de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyamg import classical\n",
    "from pyamg.classical.split import RS\n",
    "from pyamg.blackbox import make_csr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47565c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RS(matrix_A(sigma,v,w,x,b,d),second_pass=False)\n",
    "# Here A must be a csr matrix, so we use make_csr to convert matrix A into a csr matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2120f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "A_csr=make_csr(matrix_A(sigmoid,v,w,x,b,d)) #here we choose the sigmoid function as the activation function \n",
    "RS(A_csr,second_pass=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2fea29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
